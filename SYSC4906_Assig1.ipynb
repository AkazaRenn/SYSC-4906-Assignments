{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SYSC4906_Assig1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkazaRenn/SYSC-4906-Assignments/blob/master/SYSC4906_Assig1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDOT_bqlnuaL"
      },
      "source": [
        "# SYSC4906 Introduction to Machine Learning\n",
        "## Assignment 1\n",
        "|**Student name** | **Student number**|\n",
        "|-----------------|-------------------|\n",
        "| TBC | TBC |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq842SciP6bK"
      },
      "source": [
        "# Question 1\n",
        "Calculate the gradient of the following function: \n",
        "$$ f(x,z) \\stackrel{\\text{def}}{=} \\sqrt{5x^3 + z^2 + 4xz + 11x + 5} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXV6T_R8QM4l"
      },
      "source": [
        "$$ \\Delta f(x,z)= \\begin{cases}{\\delta{TBC} \\over \\delta{TBC}} \\\\{\\delta{TBC} \\over \\delta{TBC}}\\end{cases} $$\n",
        "\n",
        "$${\\delta{TBC} \\over \\delta{TBC}}= \\frac{TBC}{TBC}$$\n",
        "\n",
        "$$...$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F476OY19yL8C"
      },
      "source": [
        "#Question 2\n",
        "\n",
        "Create a python notebook which loads the sklearn breast cancer dataset (see sklearn.datasets.load_breast_cancer). This dataset has 2 classes of breast tumor: malignant and benign. There are 596 samples (212 malignant, 257 benign) and 30 features. \n",
        "1. Split the data, using 75% for training and 25% for test. Make sure you use stratified sampling. \n",
        "2. Train and test a logistic regression classifier. How accurate is your classifier?\n",
        "3. Create two subplots using the first two features from the data set. \n",
        "    \n",
        "    i)  On the first, plot the decision boundary and the training data. Use green for malignant and blue for benign.\n",
        "    \n",
        "    ii) On the second, plot the decision boundary and the test data. Use the same colours (blue/green), but highlight all misclassified test points (from either class) in red.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1-XuzEdo4ts"
      },
      "source": [
        "# Load libraries..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCZnsSnbybUQ"
      },
      "source": [
        "## Q2.1 Create the dataset\n",
        "\n",
        "The first step is loading the breast cancer data. We will then split off the test data to be used for all training sets. Then create each training set, using **stratified sampling**\n",
        "\n",
        "load_breast_cancer returns a Dictionary-like object, the interesting attributes are: \n",
        "* ‘data’, the data to learn, \n",
        "* ‘target’, the classification labels, \n",
        "* ‘target_names’, the meaning of the labels, \n",
        "* ‘feature_names’, the meaning of the features, and \n",
        "* ‘DESCR’, the full description of the dataset,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3id7ezjlsn-J"
      },
      "source": [
        "# Load the breast cancer dataset\n",
        "bcData = \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5UPspGuGNga"
      },
      "source": [
        "# Question 3\n",
        "\n",
        "Linear regression. Download the file “Assig1Q3.csv” from CULearn under “Assignments”. The first column represents the X values, while the second column represents the Y values.\n",
        "* Plot the data\n",
        "\n",
        "We are going to use linear regression to fit a linear and a quadratic model to these data. Without using sklearn.linear_model (or any other linear regression libraries), write your own python code to implement the least squares solution for linear regression. That is:\n",
        "$$\\beta=(X^TX)^{−1}X^Ty$$\n",
        "\n",
        "* Assuming the model , use your code to best-fit the parameters m and b to the data. Report your optimal parameter values. \n",
        "*Hints: \n",
        "    * recall that you must create the ‘augmented’ feature vector X from the given x data (add a column of 1’s). \n",
        "    * look at numpy.T(), numpy.matmul(), numpy.dot(), and numpy.linalg.inv()\n",
        "* Plot your line of best fit on top of the data\n",
        "* Calculate the sum of square residuals, or mean squared error, as in:\n",
        "$$MSE(\\beta) = \\sum_{i=1}^{N}{(y−X\\beta)^2}$$\n",
        "* Assuming the model , repeat steps 2-4 using this new model (i.e. estimate the optimal values for a,b,c; report those estimates; plot the line of best fit; report the MSE).\n",
        "* Briefly discuss which model would you prefer for these data?\n",
        "* Why is best-fitting the second (quadratic) model still considered linear regression?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EnFpwBiY5i4"
      },
      "source": [
        "## Step 1: Load the CSV file and best-fit a linear model using the solution derived in class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVGXLK2HZCQ6"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('Assig1Q3.csv',header=None)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-EMZyfAb89b"
      },
      "source": [
        "## Step 2: Linear model y=mx+b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVf8TwYMbx70"
      },
      "source": [
        "# Augment the x vector\n",
        "\n",
        "# Compute beta\n",
        "\n",
        "\n",
        "# Compute the MSE\n",
        "\n",
        "print(\"MSE =\", MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRN5Hhncge8y"
      },
      "source": [
        "## Step3: Quadratic model y=ax^2+bx+c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZWNuz5tge80"
      },
      "source": [
        "# Augment the x vector\n",
        "\n",
        "# Compute beta\n",
        "\n",
        "# Compute the MSE\n",
        "\n",
        "print(\"MSE =\", MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQMvagUYykXa"
      },
      "source": [
        "# Question 4\n",
        "\n",
        "- Create a Jupyter Notebook based on Lecture5.ipynb to use make_classification to create a linearly separable dataset, with 2 classes, 2 informative features, 1000 samples per class, using a class_sep=2.0, and a random_state of 3. \n",
        "- Generate some random noise of the same shape as your feature data, drawn from a standard normal distribution (see numpy.random) and a random_state of 2. \n",
        "- Create four datasets: \n",
        "    1. no noise, \n",
        "    2. data + 0.5 * noise, \n",
        "    3. data + 1.0 * noise, \n",
        "    4. data + 2.0 * noise. \n",
        "- i) For all four datasets, plot the data, labelling each (sub)plot by the degree of noise added (i.e. 0, 0.5, 1.0, and 2.0)\n",
        "- ii) For each dataset, create training and test data using a 70/30 train/test split (see train_test_split).\n",
        "- iii) For each dataset, train and test an SVM classifier with a polynomial kernel with degree=2, and C=1.0. Report the test score for each. How does prediction accuracy change with noise level?\n",
        "- iv) For a noise level of 0.5, train and test SVM classifiers using the following values for C: {0.001, 0.01, 0.1, 1, 10, 100}. \n",
        "   - Report the test accuracy for each. \n",
        "   - How does performance vary with C?\n",
        "   - Briefly describe what the C controls for sklearn.svc. *Hint: look at the documentation for sklearn.svc rather than the class notes here...*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZYt5NmSvzm"
      },
      "source": [
        "# Load the required libraries...\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}